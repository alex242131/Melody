from pathlib import Path
import pickle
from music21 import converter, instrument, note, chord, stream
from utils import get_paths
import random
import shutil


def convert_notes_to_midi(prediction_output, filename):
    """
    convert the output from the prediction to notes and create a midi file
    from the notes
    """
    offset = 0
    output_notes = list()

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ("." in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split(".")
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        elif "rest" in pattern:
            new_rest = note.Rest(pattern)
            new_rest.offset = offset
            new_rest.storedInstrument = instrument.Piano()
            output_notes.append(new_rest)
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset so notes do not stack
        offset += 0.5
    midi_stream = stream.Stream(output_notes)
    midi_stream.write("midi", filename)


def convert_midi_to_notes(midi_file):
    """ Converts midi to a list of notes """
    notes = list()
    midi = converter.parse(midi_file)
    notes_to_parse = None

    try:
        s2 = instrument.partitionByInstrument(midi)
        notes_to_parse = s2.parts[0].recurse()
    except:
        notes_to_parse = midi.flat.notesAndRests

    for element in notes_to_parse:
        if isinstance(element, note.Note):
            notes.append(str(element.pitch))
        elif isinstance(element, chord.Chord):
            notes.append(".".join(str(n) for n in element.normalOrder))
        elif isinstance(element, note.Rest):
            notes.append(element.name)
    return notes


def create_dataset(midi_files, dest_folder):
    """
    Processes the given midi files to a sequence of notes and pickles these files.
    A list of notes are returned to create a bigger vocabulary of all notes
    """
    notes = list()
    for file1 in midi_files:
        print("Processing File ", file1)
        translated_score = list()
        try:
            translated_score = convert_midi_to_notes(str(file1))
        except:
            print("Skippping {} as it is corrupted".format(file1))
            continue
        notes.extend(translated_score)
        with open(
            str(dest_folder / "{}.pkl".format(str(file1.stem))), "wb"
        ) as file_path:
            pickle.dump(translated_score, file_path)

    return notes


def main():
    paths = get_paths()
    music_files = list(paths["data_dir"].glob("*.mid"))

    # randomly select 10% test, 90%train
    # 10% train is enough in this case as these few files itself will have over 10k sequences
    # 10% train is enough in this case as these few files itself will have over 10k sequences
    random.shuffle(music_files)
    test_count = len(music_files) // 10
    # train_count = len(music_files) - test_count
    test_files = music_files[:test_count]
    train_files = music_files[test_count:]

    # create a vocab of all possible notes
    # this is the main drawback of this pre-processing approach
    # we don't know what to do when we get unseen data, how to encode?
    notes = set()

    # delete old test-train data and before generating new test-train data
    shutil.rmtree(paths["train_dir"], ignore_errors=True)
    shutil.rmtree(paths["test_dir"], ignore_errors=True)
    Path(paths["metadata_dir"]).mkdir(parents=True, exist_ok=True)
    Path(paths["test_dir"]).mkdir(parents=True, exist_ok=True)

    # create train directory
    print("Generating processed train data:\n")
    train_notes = create_dataset(train_files, dest_folder=paths["train_dir"])
    # for train we'll have a single file with all the data concatenated
    with open(str(paths["train_dir"] / "notes.pkl"), "wb") as file_path:
        pickle.dump(train_notes, file_path)
    notes |= set(train_notes)
    print("Generated train data\n\n\n")

    # create test directory
    print("Generating processed test data:\n")
    test_notes = create_dataset(test_files, dest_folder=paths["test_dir"])
    notes |= set(test_notes)
    print("Generated test data\n\n\n")

    # TODO: fix this mess later
    # create vocab here itself since doing so later might give problems while playing test data
    pitch_names = sorted(notes)
    # create a dictionary to map pitches to integers
    note_to_int = dict((note, number) for number, note in enumerate(pitch_names))
    int_to_note = dict((number, note) for number, note in enumerate(pitch_names))
    with open(paths["metadata_dir"] / "note_to_int.pkl", "wb") as f:
        pickle.dump(note_to_int, f)
    with open(paths["metadata_dir"] / "int_to_note.pkl", "wb") as f:
        pickle.dump(int_to_note, f)

    print("\nFinished Pre-processing.\n\n\n")


if __name__ == "__main__":
    main()
